---
title: '在传热学中应用 AI 的一次失败尝试'
---

最近在学传热学和深度学习，传热学的计算要查表查公式，总要翻来翻去的，表中没有的还要插值算，特别麻烦。于是突发奇想，能不能通过机器学习帮我查表呢，于是就有了以下尝试。


```python
# 试图用深度学习来查传热学的热物理性质表

import tensorflow as tf
import numpy as np

#先从书上抄几个数据
t = [-50, -40, -30, -20, -10, 0, 10, 20, 30, 40, 50]
Pr = [0.728, 0.728, 0.723, 0.716, 0.712, 0.707, 0.705, 0.703, 0.701, 0.699, 0.698]
t = np.array(t)
Pr = np.array(Pr)
print(t.shape)
print(Pr.shape)
```

    (11,)
    (11,)
    


```python
from keras import models
from keras import layers
from keras import optimizers

#建立模型
model = models.Sequential()
model.add(layers.Dense(2, input_shape=[1]))
model.add(layers.Dense(1))

model.compile(optimizer=optimizers.Adam(0.05),
             loss='mean_squared_error')

#开始训练
history = model.fit(t,
                   Pr,
                   epochs=100)
```

    Epoch 1/100
    1/1 [==============================] - 0s 999us/step - loss: 511.6551
    Epoch 2/100
    1/1 [==============================] - 0s 1000us/step - loss: 349.5498
    Epoch 3/100
    1/1 [==============================] - 0s 997us/step - loss: 229.3371
    ...
    Epoch 97/100
    1/1 [==============================] - 0s 996us/step - loss: 0.0033
    Epoch 98/100
    1/1 [==============================] - 0s 0s/step - loss: 0.0035
    Epoch 99/100
    1/1 [==============================] - 0s 997us/step - loss: 0.0036
    Epoch 100/100
    1/1 [==============================] - 0s 998us/step - loss: 0.0035
    


```python
test_t = np.array([100, 120, 140, 160, 180, 200])
test_Pr = np.array([0.688, 0.686, 0.684, 0.682, 0.681, 0.680])
#验证成果
results = model.evaluate(test_t, test_Pr)
print(model.predict(test))
print(results)
```

    1/1 [==============================] - 0s 1ms/step - loss: 0.0661
    [[0.86143714]
     [0.8761781 ]
     [0.89091855]
     [0.905659  ]
     [0.92040044]]
    0.06614607572555542
    

貌似误差有点大呢。我觉得应该有如下原因：

1. 数据太少，或者说简直是微不足道呀，但是哪里能搞到数据呢，我可不想一个个输入。
2. 模型的损失函数和优化器之类的选的不对，但我也没学到这里
3. 层数太少，这个倒是可以试一试


```python
model2 = models.Sequential()
model2.add(layers.Dense(1, input_shape=[1]))
model2.add(layers.Dense(1))
model2.add(layers.Dense(1))
model2.add(layers.Dense(1))

model2.compile(optimizer=optimizers.Adam(0.05),
             loss='mean_squared_error')

#开始训练
history = model2.fit(t,
                   Pr,
                   epochs=15)
```

    Epoch 1/15
    1/1 [==============================] - 0s 1000us/step - loss: 427.6249
    Epoch 2/15
    1/1 [==============================] - 0s 997us/step - loss: 215.8827
    Epoch 3/15
    1/1 [==============================] - 0s 997us/step - loss: 97.7123
    ...
    Epoch 13/15
    1/1 [==============================] - 0s 0s/step - loss: 18.5277
    Epoch 14/15
    1/1 [==============================] - 0s 998us/step - loss: 18.8840
    Epoch 15/15
    1/1 [==============================] - 0s 1000us/step - loss: 18.5095
    


```python
results2 = model2.evaluate(test_t, test_Pr)
print(model2.predict(test))
print(results)
```

    1/1 [==============================] - 0s 997us/step - loss: 412.5994
    [[13.871796]
     [15.194722]
     [16.517649]
     [17.840576]
     [19.163502]]
    0.06614607572555542
    

嗯......更离谱了，为什么它的预测是递增的呢，不是很明显是递减的吗......

不玩了，接着学吧，二者都才刚入门呢。
